{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Piper TTS Preprocessing Notebook\n",
        "\n",
        "This notebook preprocesses audio data for Piper TTS training.\n",
        "\n",
        "## What This Does\n",
        "1. Sets up the environment with Python 3.10 and **espeak-ng**\n",
        "2. Unzips your **wavs.zip** and copies **metadata.csv** from Google Drive\n",
        "3. Preprocesses data **locally** on Colab (very fast)\n",
        "4. **Zips the output** and saves it back to Google Drive\n",
        "\n",
        "## Requirements (in Google Drive Root)\n",
        "- `wavs.zip`: A zip file containing your wav files\n",
        "- `metadata.csv`: Transcript file `wavs/filename.wav|Text`\n",
        "\n",
        "## Important\n",
        "Run Cell 1 **twice** - it will restart the kernel after installing condacolab.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@markdown # **1. Environment Setup**\n",
        "#@markdown ---\n",
        "#@markdown Run this cell **twice**. First run installs condacolab and restarts the kernel.\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Check if condacolab is set up\n",
        "try:\n",
        "    import condacolab\n",
        "    condacolab.check()\n",
        "    print(f\"Condacolab ready! Python: {sys.version}\")\n",
        "except:\n",
        "    print(\"Installing condacolab...\")\n",
        "    !pip install -q condacolab\n",
        "    import condacolab\n",
        "    condacolab.install()  # Restarts kernel\n",
        "\n",
        "# Install Python 3.10 if needed\n",
        "if sys.version_info >= (3, 11):\n",
        "    print(\"Installing Python 3.10...\")\n",
        "    !rm -f /usr/local/conda-meta/pinned\n",
        "    !conda install -y python=3.10 --override-channels -c conda-forge -q\n",
        "    print(\"Restarting...\")\n",
        "    os.kill(os.getpid(), 9)\n",
        "\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Clone Piper\n",
        "print(\"\\nCloning Piper...\")\n",
        "!rm -rf /content/piper\n",
        "!git clone -q https://github.com/rhasspy/piper.git /content/piper\n",
        "\n",
        "# Install dependencies for preprocessing\n",
        "print(\"\\nInstalling dependencies...\")\n",
        "!sudo apt-get install -y espeak-ng\n",
        "!pip install -q cython piper-phonemize==1.1.0 librosa numpy==1.26\n",
        "\n",
        "# Build monotonic_align\n",
        "%cd /content/piper/src/python\n",
        "!bash build_monotonic_align.sh 2>/dev/null\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Setup complete!\")\n",
        "print(\"=\"*50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@markdown # **2. Preprocess Dataset**\n",
        "#@markdown ---\n",
        "#@markdown Configure paths and run preprocessing. \n",
        "#@markdown **NOTE:** This script will unzip your `wavs.zip` from Drive to the local Colab VM for maximum speed.\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "#@markdown ### Input Files (in Google Drive):\n",
        "drive_wavs_zip = \"/content/drive/MyDrive/wavs.zip\" #@param {type:\"string\"}\n",
        "drive_metadata = \"/content/drive/MyDrive/metadata.csv\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Output folder (ZIP will be saved here):\n",
        "drive_output_dir = \"/content/drive/MyDrive/colab/piper/Steve\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Model name:\n",
        "model_name = \"Steve\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ### Language:\n",
        "language = \"en-us\" #@param [\"en-us\", \"en\", \"de\", \"fr\", \"es\", \"it\", \"pt-br\", \"nl\", \"pl\", \"ru\", \"zh\"]\n",
        "\n",
        "#@markdown ### Sample rate:\n",
        "sample_rate = \"22050\" #@param [\"16000\", \"22050\"]\n",
        "\n",
        "#@markdown ### Single speaker dataset?\n",
        "single_speaker = True #@param {type:\"boolean\"}\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# Setup Local Directories\n",
        "# ----------------------------------------------------------\n",
        "local_input_dir = \"/content/dataset_input\"\n",
        "local_output_dir = \"/content/piper_preprocessed\"\n",
        "cache_dir = \"/content/audio_cache\"\n",
        "\n",
        "# Clean up previous runs if needed\n",
        "for d in [local_input_dir, local_output_dir, cache_dir]:\n",
        "    if os.path.exists(d):\n",
        "        shutil.rmtree(d)\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"Preparing Input Data (Local Copy)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 1. Unzip wavs\n",
        "print(f\"[1/3] Unzipping wavs from {drive_wavs_zip}...\")\n",
        "if not os.path.exists(drive_wavs_zip):\n",
        "    raise Exception(f\"wavs file not found at: {drive_wavs_zip}\")\n",
        "\n",
        "!unzip -q \"{drive_wavs_zip}\" -d \"{local_input_dir}\"\n",
        "\n",
        "# Handle case where zip contains a folder (e.g. wavs/file.wav) or just files\n",
        "# We need the structure local_input_dir/wavs/file.wav\n",
        "extracted_contents = os.listdir(local_input_dir)\n",
        "if \"wavs\" in extracted_contents and os.path.isdir(os.path.join(local_input_dir, \"wavs\")):\n",
        "    print(\"      dataset structure looks correct (found wavs folder).\")\n",
        "else:\n",
        "    print(\"      'wavs' folder not found directly in zip. Creating it...\")\n",
        "    # If the user just zipped a bunch of wav files without a parent 'wavs' folder\n",
        "    wavs_path = os.path.join(local_input_dir, \"wavs\")\n",
        "    os.makedirs(wavs_path, exist_ok=True)\n",
        "    for f in extracted_contents:\n",
        "        src = os.path.join(local_input_dir, f)\n",
        "        if os.path.isfile(src) and f.lower().endswith(\".wav\"):\n",
        "            shutil.move(src, os.path.join(wavs_path, f))\n",
        "\n",
        "# 2. Copy metadata\n",
        "print(f\"[2/3] Copying metadata from {drive_metadata}...\")\n",
        "if not os.path.exists(drive_metadata):\n",
        "    raise Exception(f\"Metadata file not found at: {drive_metadata}\")\n",
        "\n",
        "shutil.copy2(drive_metadata, os.path.join(local_input_dir, \"metadata.csv\"))\n",
        "\n",
        "# 3. Run Preprocessing\n",
        "print(f\"[3/3] Running Piper processing...\")\n",
        "speaker_flag = \"--single-speaker\" if single_speaker else \"\"\n",
        "\n",
        "%cd /content/piper/src/python\n",
        "\n",
        "!python -m piper_train.preprocess \\\n",
        "    --language {language} \\\n",
        "    --input-dir \"{local_input_dir}\" \\\n",
        "    --output-dir \"{local_output_dir}\" \\\n",
        "    --dataset-name \"{model_name}\" \\\n",
        "    --dataset-format ljspeech \\\n",
        "    --sample-rate {sample_rate} \\\n",
        "    --cache-dir \"{cache_dir}\" \\\n",
        "    {speaker_flag}\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Preprocessing complete! Packaging results...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Zip results\n",
        "zip_name = f\"{model_name}_preprocessed.zip\"\n",
        "zip_path = os.path.join(\"/content\", zip_name)\n",
        "print(f\"Zipping to {zip_path}...\")\n",
        "!cd \"{local_output_dir}\" && zip -r -q \"{zip_path}\" .\n",
        "\n",
        "# Copy to Drive\n",
        "os.makedirs(drive_output_dir, exist_ok=True)\n",
        "drive_dest = os.path.join(drive_output_dir, zip_name)\n",
        "print(f\"Copying to Google Drive: {drive_dest}\")\n",
        "shutil.copy2(zip_path, drive_dest)\n",
        "\n",
        "print(\"\\nDONE! Download the zip file from your Drive to train locally.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#@markdown # **3. Verify Preprocessing (from Local Temp)**\n",
        "#@markdown ---\n",
        "#@markdown Check that all required files were created in the local temp folder.\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "# We check the local temp dir where data was just generated\n",
        "check_dir = local_output_dir\n",
        "\n",
        "print(f\"Checking output in: {check_dir}\\n\")\n",
        "\n",
        "# Check config.json\n",
        "config_path = os.path.join(check_dir, \"config.json\")\n",
        "if os.path.exists(config_path):\n",
        "    print(\"[OK] config.json exists\")\n",
        "    with open(config_path) as f:\n",
        "        config = json.load(f)\n",
        "        print(f\"     Sample rate: {config.get('audio', {}).get('sample_rate', 'N/A')}\")\n",
        "else:\n",
        "    print(\"[ERROR] config.json missing!\")\n",
        "\n",
        "# Check dataset.jsonl\n",
        "dataset_path = os.path.join(check_dir, \"dataset.jsonl\")\n",
        "if os.path.exists(dataset_path):\n",
        "    with open(dataset_path) as f:\n",
        "        lines = f.readlines()\n",
        "    print(f\"[OK] dataset.jsonl exists ({len(lines)} entries)\")\n",
        "    \n",
        "    # Check first entry for required fields\n",
        "    if len(lines) > 0:\n",
        "        first = json.loads(lines[0])\n",
        "        has_audio_norm = \"audio_norm_path\" in first\n",
        "        has_phoneme_ids = \"phoneme_ids\" in first\n",
        "        print(f\"     Has audio_norm_path: {has_audio_norm}\")\n",
        "        print(f\"     Has phoneme_ids: {has_phoneme_ids}\")\n",
        "        \n",
        "        if not has_audio_norm:\n",
        "            print(\"\\n[WARNING] audio_norm_path missing - audio cache may not have been created\")\n",
        "else:\n",
        "    print(\"[ERROR] dataset.jsonl missing!\")\n",
        "\n",
        "# Check audio cache (in output)\n",
        "audio_dir = os.path.join(check_dir, \"audio\")\n",
        "if os.path.exists(audio_dir):\n",
        "    pt_files = [f for f in os.listdir(audio_dir) if f.endswith('.pt')]\n",
        "    print(f\"[OK] audio/ folder exists ({len(pt_files)} .pt files)\")\n",
        "else:\n",
        "    print(\"[WARNING] audio/ folder missing! (Did preprocessing finish?)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Verification complete!\")\n",
        "print(\"=\"*50)\n",
        "print(f\"\\nThe ZIP file should be in your Drive at: {drive_dest}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}